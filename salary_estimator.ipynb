{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQMuEgU0LiwxVj0etuaxr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiorgosNik/dev-salary-estimator/blob/main/salary_estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "_K12Bkun-T0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install -U -q geocoder\n",
        "!pip install -U -q tqdm\n",
        "!pip install -U -q tensorflow"
      ],
      "metadata": {
        "id": "ZSFaceekf-a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apXdmT4L8GOD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from geopy.geocoders import Nominatim\n",
        "from tqdm import tqdm\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# CSV import from Google Drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Format Data"
      ],
      "metadata": {
        "id": "u6lmA34W-XqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import from Google Drive\n",
        "Authenticate with GoogleAuth to retrieve the data .csv from Google Drive.\n",
        "The retrieved .csv file is stored in a pandas dataframe."
      ],
      "metadata": {
        "id": "bb3MZdj5kHiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# ID of Google Drive .csv document\n",
        "id = \"1cPsSR9XfyqOl15KqGV3BEqaxmL1R7lqq\"\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('data.csv')  \n",
        "dataframe = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "M_B51PCL9fiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.columns = ['timestamp', 'devtype', 'languages', 'years_experience','personal_projects', 'sex', 'remote','city_residence', 'city_work', 'company_size',  'supervisor', 'education', 'relevant', 'salary']\n",
        "\n",
        "# Remove the timestamp as it is not relevant\n",
        "dataframe = dataframe.drop(columns=['timestamp'])\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "dataset_size = len(dataframe)\n",
        "print(\"The dataset contains {} salary entries\".format( dataset_size))\n",
        "dataframe.head(2)"
      ],
      "metadata": {
        "id": "q4PV5g-79-AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove entries that contain very rare developer types or languages\n",
        "Entries that occur under 5 times are considered rare."
      ],
      "metadata": {
        "id": "BasxDZwQmEhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNK = 'unk'\n",
        "\n",
        "def categorize_clean_columns(df, colname, threshold):\n",
        "    df[colname] = df[colname].map(lambda x: x.replace(' ', '').split(','))\n",
        "    c = Counter([item for sublist in df[colname].values for item in sublist])\n",
        "\n",
        "    map_to_unk = set([el for el in c.elements() if c[el] <= threshold])\n",
        "\n",
        "    def map_devtype_unk(arr):\n",
        "      def x_or_unk(x):\n",
        "        if x in map_to_unk:\n",
        "          return UNK\n",
        "        return x\n",
        "\n",
        "      return [x_or_unk(x) for x in arr]\n",
        "\n",
        "    df[colname] = df[colname].map(lambda x: map_devtype_unk(x))\n",
        "\n",
        "    def is_sole_uknown(arr):\n",
        "      return arr[0] == UNK and len(arr) == 1\n",
        "\n",
        "    return df[df[colname].map(is_sole_uknown) == False]"
      ],
      "metadata": {
        "id": "3Oqupvawe4Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_devtype(devtypes):\n",
        "    return list(filter(lambda value: value != UNK, devtypes))\n",
        "\n",
        "def fix_languages(languages):\n",
        "    for i in range(len(languages)):\n",
        "      if(languages[i] == \"Typescript\"):\n",
        "        languages[i] = \"TypeScript\"\n",
        "      languages[i] = languages[i].strip()\n",
        "    return list(filter(lambda value: value != UNK, languages))"
      ],
      "metadata": {
        "id": "hV5PbxCy1TfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = categorize_clean_columns(dataframe, 'devtype', 5)\n",
        "dataframe = categorize_clean_columns(dataframe, 'languages', 5)\n",
        "\n",
        "dataframe['languages'] = dataframe['languages'].map(lambda x: fix_languages(x))\n",
        "dataframe['devtype'] = dataframe['devtype'].map(lambda x: fix_devtype(x))\n",
        "\n",
        "dataframe = dataframe[dataframe['languages'].map(lambda d: len(d)) > 0]\n",
        "dataframe[dataframe['devtype'].map(lambda d: len(d)) > 0]\n",
        "\n",
        "print(\"After removing rare entries, the dataset contains {} entries.\".format(len(dataframe)))\n",
        "print(\"This is {} less than the initial dataser.\".format(dataset_size - len(dataframe)))"
      ],
      "metadata": {
        "id": "8g-DYWIni0y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix Salary\n",
        "Normalize the salary input. Remove €/$ signs, commas and period signs.\n",
        "For values under 4000, consider the user gave a montly salary by mistake, and calculate the yearly salary by multiplying by 14 salaries.  "
      ],
      "metadata": {
        "id": "Yw5o6JNzw4tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_salary(salary):\n",
        "    salary = salary.replace('.', '')\n",
        "    salary = salary.replace(',', '')\n",
        "    salary = salary.replace('€', '')\n",
        "    salary = salary.replace('$', '')\n",
        "    salary = int(salary)\n",
        "    if salary < 4000:\n",
        "      salary = salary * 14\n",
        "    \n",
        "    return salary\n",
        "\n",
        "dataframe['salary'] = dataframe['salary'].map(lambda x: fix_salary(str(int(x)))).astype('int32')\n",
        "dataframe.head(5)"
      ],
      "metadata": {
        "id": "pE5kbqqzmhLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consider only results from Greece\n",
        "Using geopy, use the given city names to obtain the country of each response.\n",
        "Reject all values not relevant to Greece due to low sample size."
      ],
      "metadata": {
        "id": "ddsh8w5_yh7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cities_residence, cities_work = pd.unique(dataframe['city_residence'].values), pd.unique(dataframe['city_work'].values)\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"test\")\n",
        "\n",
        "with tqdm(total=len(cities_residence), desc='Formatting City Names') as city_progressbar:\n",
        "  for city in cities_residence:\n",
        "    try:\n",
        "      country = geolocator.geocode(city).address.split(\",\")[-1].strip()\n",
        "      dataframe['city_residence'] = dataframe['city_residence'].replace(city,country)\n",
        "\n",
        "    except AttributeError:\n",
        "      dataframe['city_residence'] = dataframe['city_residence'].replace(city,\"UNK\")\n",
        "    city_progressbar.update(1)\n",
        "\n",
        "dataframe = dataframe[dataframe.city_residence == \"Ελλάς\"]\n",
        "\n",
        "dataframe = dataframe.drop(columns=['city_residence', 'city_work'])\n",
        "dataframe.head(2)"
      ],
      "metadata": {
        "id": "oUCaylBYotjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace NA values\n",
        "Replace NA values regarding relevant projects with \"No\""
      ],
      "metadata": {
        "id": "WGqSrT3zzPQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['relevant'] = dataframe['relevant'].fillna('Όχι')"
      ],
      "metadata": {
        "id": "7tTAeQUGeE5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Feature Columns"
      ],
      "metadata": {
        "id": "rw0-1Sgxzei4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_columns = ['company_size', 'remote', 'supervisor', 'personal_projects', 'sex','education','relevant']\n",
        "\n",
        "for col in category_columns:\n",
        "    dataframe[f'{col}_xf'] = dataframe[col].astype('category')\n",
        "\n",
        "dataframe = dataframe.drop(columns=category_columns)"
      ],
      "metadata": {
        "id": "ZMfzC-xtfj7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_category_columns = ['devtype', 'languages']\n",
        "\n",
        "def coltitle(col, word):\n",
        "    return f'{col}_{word}'\n",
        "\n",
        "for col in multi_category_columns:\n",
        "    vocab = set([item for sublist in dataframe[col].values for item in sublist])\n",
        "    for word in vocab:\n",
        "        dataframe[coltitle(col, word)] = 0\n",
        "        dataframe[coltitle(col, word)] = dataframe[coltitle(col, word)].astype('int32')\n",
        "    print(vocab)\n",
        "\n",
        "for index, row in dataframe.iterrows():\n",
        "    for col in multi_category_columns:\n",
        "        words = row[col]\n",
        "        for word in words:\n",
        "            dataframe.loc[index, coltitle(col, word)] = 1\n",
        "\n",
        "bad_tf_scope_names = [('languages_C#', 'languages_Csharp'), ('languages_C++', 'languages_Cpp')]\n",
        "for before, after in bad_tf_scope_names:\n",
        "    dataframe[after] = dataframe[before]\n",
        "    dataframe = dataframe.drop(columns=[before])\n",
        "\n",
        "dataframe = dataframe.drop(columns=multi_category_columns)\n"
      ],
      "metadata": {
        "id": "_e7GFD_hgY89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into training and validation sets"
      ],
      "metadata": {
        "id": "lEhwFrJ52J-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(dataframe, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')"
      ],
      "metadata": {
        "id": "CL8C644Pgs2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop('salary')\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n",
        "\n",
        "train_ds = df_to_dataset(train, batch_size=32)\n",
        "eval_ds = df_to_dataset(train, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "FbjuD0nGg2lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  feature_column_keys = list(feature_batch.keys())\n",
        "  print('Every feature:', feature_column_keys)"
      ],
      "metadata": {
        "id": "ky-qm-mwg9Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode features into columns"
      ],
      "metadata": {
        "id": "HD49BSqe2VVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns = []\n",
        "\n",
        "numeric_features = [x for x in feature_column_keys if ('devtype_' in x or 'languages_' in x or 'years_experience' in x)]\n",
        "categorical_features = [x for x in feature_column_keys if '_xf' in x]\n",
        "\n",
        "for feature in numeric_features:\n",
        "    feature_columns.append(feature_column.numeric_column(feature))\n",
        "\n",
        "for feature in categorical_features:\n",
        "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
        "      feature, pd.unique(dataframe[feature].values))\n",
        "\n",
        "    feature_columns.append(feature_column.indicator_column(categorical_column))\n",
        "\n",
        "print(feature_columns)"
      ],
      "metadata": {
        "id": "oTKmYnRuhALW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "ClxgUGOR2eE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(8, activation='relu'),  \n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(momentum=0.9),\n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]\n",
        "print(train_ds)\n",
        "model.fit(train_ds,\n",
        "          validation_data=eval_ds,\n",
        "          epochs=100,\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "id": "AcKwFaXjhB8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try your values"
      ],
      "metadata": {
        "id": "83SIIyqL2gzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "    'years_experience': 1,\n",
        "    'company_size_xf': '11-50',\n",
        "    'education_xf': \"Bachelor's\",\n",
        "    'relevant_xf': \"Ναι\",\n",
        "    'personal_projects_xf': 'Ναι',\n",
        "    'remote_xf': 'Και τα δύο',\n",
        "    'supervisor_xf': 'Ναι',\n",
        "    'sex_xf': 'Άντρας',\n",
        "    'devtype_Backend': 1 ,              \n",
        "    'devtype_Desktopapps': 0,         \n",
        "    'devtype_DevOps': 1,            \n",
        "    'devtype_AI/ML': 0 ,       \n",
        "    'devtype_BI': 0 ,\n",
        "    'devtype_Cybersecurity': 0,\n",
        "    'devtype_Embedded': 0,\n",
        "    'devtype_Gaming': 0,                     \n",
        "    'devtype_Frontend': 1,        \n",
        "    'devtype_Mobileapps': 0,           \n",
        "    'languages_C': 0 ,\n",
        "    'languages_SQL': 0,              \n",
        "    'languages_PHP': 0 ,             \n",
        "    'languages_JavaScript': 1 ,         \n",
        "    'languages_Kotlin' : 0 ,\n",
        "    'languages_TypeScript' : 0,              \n",
        "    'languages_Python': 1 ,            \n",
        "    'languages_Ruby': 0 ,               \n",
        "    'languages_Bash': 0,             \n",
        "    'languages_Go': 0 ,             \n",
        "    'languages_Java': 0 ,            \n",
        "    'languages_Swift': 0 ,            \n",
        "    'languages_Csharp': 0 ,         \n",
        "    'languages_Cpp': 0 , \n",
        "}\n",
        "\n",
        "input = {k: [v] for k, v in input.items()}\n",
        "\n",
        "prediction = model(input).numpy()[0][0]\n",
        "f'Βγάζεις: {prediction} ευρώ το χρόνο'"
      ],
      "metadata": {
        "id": "sOCTrzYphmL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}